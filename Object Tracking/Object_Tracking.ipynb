{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Object Tracking\n",
        "## Computer Vision\n"
      ],
      "metadata": {
        "id": "4B4006jNpb53"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Detection\n",
        "\n",
        "Use a pre-trained YOLOv5 model to detect people in each frame of the video. The model should identify the bounding boxes around each person along with the confidence scores."
      ],
      "metadata": {
        "id": "7RNaGHCkpkH7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ],
      "metadata": {
        "id": "fyrNBF9wkKtU"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yHqsHPJQbaF0",
        "outputId": "679c542f-5ddf-49cb-c456-3f26e6b725ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'yolov5' already exists and is not an empty directory.\n",
            "/content/yolov5\n",
            "Requirement already satisfied: gitpython>=3.1.30 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 5)) (3.1.43)\n",
            "Requirement already satisfied: matplotlib>=3.3 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 6)) (3.7.1)\n",
            "Requirement already satisfied: numpy>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 7)) (1.26.4)\n",
            "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 8)) (4.10.0.84)\n",
            "Requirement already satisfied: pillow>=10.3.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 9)) (10.4.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 10)) (5.9.5)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 11)) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.32.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 12)) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 13)) (1.13.1)\n",
            "Requirement already satisfied: thop>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 14)) (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 15)) (2.4.0+cu121)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 16)) (0.19.0+cu121)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 17)) (4.66.5)\n",
            "Requirement already satisfied: ultralytics>=8.2.34 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 18)) (8.2.93)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 27)) (2.1.4)\n",
            "Requirement already satisfied: seaborn>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 28)) (0.13.1)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.10/dist-packages (from -r requirements.txt (line 42)) (71.0.4)\n",
            "Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.10/dist-packages (from gitpython>=3.1.30->-r requirements.txt (line 5)) (4.0.11)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.3.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (4.53.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (1.4.7)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (24.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (3.1.4)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=3.3->-r requirements.txt (line 6)) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0->-r requirements.txt (line 12)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0->-r requirements.txt (line 12)) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0->-r requirements.txt (line 12)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.0->-r requirements.txt (line 12)) (2024.8.30)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.16.0)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (1.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.8.0->-r requirements.txt (line 15)) (2024.6.1)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (9.0.0)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from ultralytics>=8.2.34->-r requirements.txt (line 18)) (2.0.6)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.1.4->-r requirements.txt (line 27)) (2024.1)\n",
            "Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from gitdb<5,>=4.0.1->gitpython>=3.1.30->-r requirements.txt (line 5)) (5.0.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3->-r requirements.txt (line 6)) (1.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.8.0->-r requirements.txt (line 15)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.8.0->-r requirements.txt (line 15)) (1.3.0)\n",
            "fatal: destination path 'Yolov5_DeepSort_Pytorch' already exists and is not an empty directory.\n",
            "/content/yolov5/Yolov5_DeepSort_Pytorch\n",
            "\u001b[31mERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\u001b[0m\u001b[31m\n",
            "\u001b[0m/content/yolov5\n"
          ]
        }
      ],
      "source": [
        "# Clone the YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5\n",
        "%cd yolov5\n",
        "\n",
        "# Install dependencies for YOLOv5\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Clone the DeepSORT repository\n",
        "!git clone https://github.com/mikel-brostrom/Yolov5_DeepSort_Pytorch.git\n",
        "%cd Yolov5_DeepSort_Pytorch\n",
        "\n",
        "# Install dependencies for DeepSORT\n",
        "!pip install -r requirements.txt\n",
        "\n",
        "# Go back to the main directory\n",
        "%cd .."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install deep-sort-realtime"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yt-g8AvIc3zs",
        "outputId": "66141d50-2a55-4a37-ea33-1d31e5c2100b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: deep-sort-realtime in /usr/local/lib/python3.10/dist-packages (1.3.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (1.26.4)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (1.13.1)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.10/dist-packages (from deep-sort-realtime) (4.10.0.84)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Object Tracking\n",
        "\n",
        "Implement DeepSORT to track each detected person across frames. Each person should be assigned a unique ID that remains consistent throughout their appearance in the video.\n",
        "\n"
      ],
      "metadata": {
        "id": "jNc-h6Dtps0Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Tracking Output\n",
        "\n",
        "Display the video with bounding boxes around each tracked person. Each bounding box should be labeled with the corresponding track ID."
      ],
      "metadata": {
        "id": "bU7iFXDJqD4s"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "\n",
        "# Load YOLOv5 model (small version for speed)\n",
        "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Video path and output setup\n",
        "video_path = \"person.mp4\"  # Ensure this file is uploaded\n",
        "output_video_path = \"output_detected_people.mp4\"\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# VideoWriter to save the output video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Function to draw bounding boxes\n",
        "def draw_bboxes(img, bbox, confidences):\n",
        "    for i, box in enumerate(bbox):\n",
        "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
        "        conf = confidences[i]\n",
        "        color = (255, 0, 0)\n",
        "        label = f'Person {conf:.2f}'\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
        "    return img\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Apply YOLOv5 to detect people in the frame\n",
        "    results = yolo_model(frame)\n",
        "\n",
        "    # Filter only 'person' detections (class 0 in YOLOv5)\n",
        "    bbox_xyxy = []\n",
        "    confidences = []\n",
        "    for detection in results.xyxy[0]:\n",
        "        if int(detection[-1]) == 0:  # class 0 corresponds to 'person'\n",
        "            x1, y1, x2, y2, conf, _ = detection\n",
        "            bbox_xyxy.append([x1.item(), y1.item(), x2.item(), y2.item()])\n",
        "            confidences.append(conf.item())\n",
        "\n",
        "    # Draw bounding boxes with confidence scores\n",
        "    if bbox_xyxy:\n",
        "        frame = draw_bboxes(frame, bbox_xyxy, confidences)\n",
        "\n",
        "    # Write the frame with bounding boxes to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release everything\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# After processing, download the video file from Colab\n",
        "from google.colab import files\n",
        "files.download(output_video_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "id": "vp_fMU7ob7ZV",
        "outputId": "9d491108-0073-4871-c5cc-e07a7631d7b7"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2024-9-13 Python-3.10.12 torch-2.4.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_605413be-5f8b-4d31-aaf5-c09909a21eb0\", \"output_detected_people.mp4\", 27096382)"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import cv2\n",
        "from deep_sort_realtime.deepsort_tracker import DeepSort\n",
        "\n",
        "# Load YOLOv5 model (small version for speed)\n",
        "yolo_model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "\n",
        "# Initialize DeepSORT\n",
        "deepsort = DeepSort(max_age=30, nn_budget=70, max_iou_distance=0.7)\n",
        "\n",
        "# Video path and output setup\n",
        "video_path = \"person.mp4\"  # Ensure this file is uploaded\n",
        "output_video_path = \"output_tracked_people.mp4\"\n",
        "\n",
        "# Open the video file\n",
        "cap = cv2.VideoCapture(video_path)\n",
        "width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "\n",
        "# VideoWriter to save the output video\n",
        "fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "out = cv2.VideoWriter(output_video_path, fourcc, fps, (width, height))\n",
        "\n",
        "# Function to draw bounding boxes with track IDs\n",
        "def draw_bboxes(img, bbox, identities=None, offset=(0, 0)):\n",
        "    for i, box in enumerate(bbox):\n",
        "        x1, y1, x2, y2 = [int(coord) for coord in box]\n",
        "        id = int(identities[i]) if identities is not None else 0\n",
        "        color = (255, 0, 0)\n",
        "        label = '{}{:d}'.format(\"ID \", id)\n",
        "        cv2.rectangle(img, (x1, y1), (x2, y2), color, 2)\n",
        "        cv2.putText(img, label, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.75, color, 2)\n",
        "    return img\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "\n",
        "    # Apply YOLOv5 to detect people in the frame\n",
        "    results = yolo_model(frame)\n",
        "\n",
        "    # Filter only 'person' detections (class 0 in YOLOv5)\n",
        "    detections = []\n",
        "    for detection in results.xyxy[0]:\n",
        "        if int(detection[-1]) == 0:  # class 0 corresponds to 'person'\n",
        "            x1, y1, x2, y2, conf, _ = detection\n",
        "            detections.append(([x1.item(), y1.item(), x2.item(), y2.item()], conf.item()))  # bbox and confidence\n",
        "\n",
        "    # If there are detections, pass them to DeepSORT for tracking\n",
        "    if detections:\n",
        "        tracks = deepsort.update_tracks(detections, frame=frame)  # Call DeepSORT\n",
        "\n",
        "        # Collect bounding boxes and identities from the tracks\n",
        "        bbox_xyxy = []\n",
        "        identities = []\n",
        "        for track in tracks:\n",
        "            if not track.is_confirmed():\n",
        "                continue\n",
        "            bbox = track.to_ltrb()  # Get bbox coordinates (left, top, right, bottom)\n",
        "            id = track.track_id  # Get track ID\n",
        "            bbox_xyxy.append(bbox)\n",
        "            identities.append(id)\n",
        "\n",
        "        # Draw bounding boxes with track IDs\n",
        "        frame = draw_bboxes(frame, bbox_xyxy, identities)\n",
        "\n",
        "    # Write the frame with bounding boxes to the output video\n",
        "    out.write(frame)\n",
        "\n",
        "# Release everything\n",
        "cap.release()\n",
        "out.release()\n",
        "\n",
        "# After processing, download the video file from Colab\n",
        "from google.colab import files\n",
        "files.download(output_video_path)"
      ],
      "metadata": {
        "id": "jG7CMFzQjqzm",
        "outputId": "f61396d0-c617-4e38-de42-51157d1648b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2024-9-13 Python-3.10.12 torch-2.4.0+cu121 CUDA:0 (Tesla T4, 15102MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_6a115d32-f0cd-420a-868e-39ad8cf3257b\", \"output_tracked_people.mp4\", 32286293)"
            ]
          },
          "metadata": {}
        }
      ]
    }
  ]
}